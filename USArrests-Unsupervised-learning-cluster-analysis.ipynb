{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/hmahida/usarrests-pca-k-means-and-agglomerative-clustering?scriptVersionId=118595052\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nos.environ[\"OMP_NUM_THREADS\"] = '1' # To avoid KMeans memory leak on Windows with MKL userwarning\n\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2023-01-23T00:06:16.109745Z","iopub.execute_input":"2023-01-23T00:06:16.110139Z","iopub.status.idle":"2023-01-23T00:06:16.115836Z","shell.execute_reply.started":"2023-01-23T00:06:16.110108Z","shell.execute_reply":"2023-01-23T00:06:16.114691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### This data set contains statistics, in arrests per 100,000 residents for assault, murder, and rape in each of the 50 US states in 1973. Also given is the percent of the population living in urban areas.","metadata":{}},{"cell_type":"code","source":"# load data\ndf =  pd.read_csv('/kaggle/input/d/hmahida/usarrests/UsArrests.csv', index_col=0)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-01-23T00:06:16.117875Z","iopub.execute_input":"2023-01-23T00:06:16.118312Z","iopub.status.idle":"2023-01-23T00:06:16.1427Z","shell.execute_reply.started":"2023-01-23T00:06:16.118269Z","shell.execute_reply":"2023-01-23T00:06:16.141338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info(), df.describe()","metadata":{"execution":{"iopub.status.busy":"2023-01-23T00:06:16.144812Z","iopub.execute_input":"2023-01-23T00:06:16.145607Z","iopub.status.idle":"2023-01-23T00:06:16.189473Z","shell.execute_reply.started":"2023-01-23T00:06:16.145557Z","shell.execute_reply":"2023-01-23T00:06:16.188343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### From the dataset summary above the following observations can be made\n- The dataset contains 50 rows and 5 columns.\n- The four variables have vastly different means\n- The variables also have vastly different variances \n- UrbanPop variable measures the percentage of the population in each state living in an urban area, which is not a comparable number to the number of murders, rapes or assaults in each state per 100,000 individuals.\n- There are no null values to report in the dataset\n","metadata":{}},{"cell_type":"code","source":"# Plot histograms\ndf.hist(grid=False, figsize=(10, 5))\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-01-23T00:06:16.191175Z","iopub.execute_input":"2023-01-23T00:06:16.191971Z","iopub.status.idle":"2023-01-23T00:06:16.919677Z","shell.execute_reply.started":"2023-01-23T00:06:16.191926Z","shell.execute_reply":"2023-01-23T00:06:16.918718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Crime Rate & Urban Population","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10,30))\ny = np.arange(len(df.axes[0]))  # the label locations\nbar_height = 0.4  # the height of the bars\n\n# set the position of the bars on the y-axis\nbar_positions = y - bar_height\n\nassult = ax.barh(bar_positions, df.Assault, bar_height, color = 'g')\nrape = ax.barh(bar_positions, df.Rape, bar_height, color = 'b', left=df.Assault)\nmurder = ax.barh(bar_positions, df.Murder, bar_height, color = 'r', left=df.Assault+df.Rape)\nurbanpop = ax.barh(bar_positions + bar_height, df.UrbanPop, bar_height, color = 'cyan')\n\nax.set_yticks(y)  # set the y-ticks to be at the same position as the bars\nax.set_yticklabels(df.axes[0])  # set the y-tick labels to be the labels of the dataframe\n\nplt.legend(['Assault','Rape','Murder','UrbanPop'])\nplt.margins(y=0)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-01-23T00:06:16.922657Z","iopub.execute_input":"2023-01-23T00:06:16.923847Z","iopub.status.idle":"2023-01-23T00:06:18.183392Z","shell.execute_reply.started":"2023-01-23T00:06:16.923795Z","shell.execute_reply":"2023-01-23T00:06:18.182511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### From the bar chart above the following observations can be made\n- Highest Assualt Rate : Florida and North California.\n- Lowest Assualt Rate : Hawaii, North Dakota, Vermont , New Hampshire and Wisconsin.\n\n- Highest Rape Rate : Nevada and Alaska.\n- Lowest Rape Rate : Maine, North Dakota,Vermont,Connecticut,New Hampshire, Wisconsin,Rhode Island and West Virginia\n\n- Highest Murder Rate : Georgia and Missisippi\n- Lowest Murder Rate : Idaho , Iowa, Maine, New Hampshire, North Dakota, Vermont and Wisconsin.\n\n- Highest UrbanPop Rate : Nevada and Alaska.\n- Lowest UrbanPop Rate : Maine, North Dakota,Vermont,Connecticut,New Hampshire, Wisconsin,Rhode Island and West Virginia\n","metadata":{"jp-MarkdownHeadingCollapsed":true,"tags":[]}},{"cell_type":"markdown","source":"### Correlation Analysis","metadata":{}},{"cell_type":"code","source":"states = df.index\ncorr_df = df.corr()\nlabels = corr_df.columns\n\nmask_ut=np.triu(np.ones(corr_df.shape)).astype(bool)\nsns.heatmap(corr_df, mask=mask_ut, annot=True, cmap=\"coolwarm\")","metadata":{"execution":{"iopub.status.busy":"2023-01-23T00:06:18.184687Z","iopub.execute_input":"2023-01-23T00:06:18.18518Z","iopub.status.idle":"2023-01-23T00:06:18.498036Z","shell.execute_reply.started":"2023-01-23T00:06:18.185149Z","shell.execute_reply":"2023-01-23T00:06:18.496597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### From the heat map above the following observations can be made\n- Rate of arrests for assault has very strong positive correlation with the rate of arrests for murder.\n- Rate of arrests for assault has strong positive correlation with the rate of arrests for rape.\n- Rate of arrests for murder has moderate positive correlation with the rate of arrests for rape.\n- Urbanpopulation percentage has moderate positive correlation with the rate of arrests for rape.\n- Urbanpopulation percentage has weak positive correlation with the rate of arrests for assault.\n- Urbanpopulation percentage has almost no correlation with the rate of arrests for murder.","metadata":{}},{"cell_type":"code","source":"sns.pairplot(df, kind='reg')","metadata":{"execution":{"iopub.status.busy":"2023-01-23T00:06:18.500389Z","iopub.execute_input":"2023-01-23T00:06:18.501142Z","iopub.status.idle":"2023-01-23T00:06:22.842928Z","shell.execute_reply.started":"2023-01-23T00:06:18.501096Z","shell.execute_reply":"2023-01-23T00:06:22.841615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### From the pairplots above the following observations can be made\n- Murder , Assault & Rape are highly co-related to each other. \n- UrbanPop is not in co-relation with other variables. \n- Because of the highly co-related input data, PCA can be applied to reduce the number of features.","metadata":{"tags":[]}},{"cell_type":"markdown","source":"## PCA","metadata":{}},{"cell_type":"code","source":"from sklearn.decomposition import PCA\nnp.set_printoptions(precision=2)\n\nX = df.values.squeeze()\n\npca = PCA()\n\nX_trans = pca.fit_transform(X)\n\ndf_pca = pd.DataFrame(X_trans)\ndf_pca.head()","metadata":{"execution":{"iopub.status.busy":"2023-01-23T00:06:22.844542Z","iopub.execute_input":"2023-01-23T00:06:22.84497Z","iopub.status.idle":"2023-01-23T00:06:23.097331Z","shell.execute_reply.started":"2023-01-23T00:06:22.844926Z","shell.execute_reply":"2023-01-23T00:06:23.095797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"std = df_pca.describe().transpose()[\"std\"]\nprint(f\"Standard deviation: {std.values}\")\nprint(f\"Proportion of Variance Explained: {pca.explained_variance_ratio_}\")\nprint(f\"Cumulative Proportion: {np.cumsum(pca.explained_variance_)}\")","metadata":{"execution":{"iopub.status.busy":"2023-01-23T00:06:23.098595Z","iopub.execute_input":"2023-01-23T00:06:23.098924Z","iopub.status.idle":"2023-01-23T00:06:23.121868Z","shell.execute_reply.started":"2023-01-23T00:06:23.098897Z","shell.execute_reply":"2023-01-23T00:06:23.120645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def biplot(score,coeff,labels=None,points=None):\n    xs = score[:,0]\n    ys = score[:,1]\n    n = coeff.shape[0]\n    scalex = 1.0/(xs.max() - xs.min())\n    scaley = 1.0/(ys.max() - ys.min())\n\n    fig, ax = plt.subplots(figsize=(15, 15))    \n\n    ax.scatter(xs * scalex,ys * scaley,s=5)\n\n    for i in range(0,len(xs)):\n        txt = states[i]\n        ax.annotate(txt, (xs[i]* scalex, ys[i]* scaley))\n\n    for i in range(n):\n        ax.arrow(0, 0, coeff[i,0], coeff[i,1],color = 'r',alpha = 0.5)\n        if labels is None:\n            ax.text(coeff[i,0]* 1.15, coeff[i,1] * 1.15, \"Var\"+str(i+1), color = 'green', ha = 'center', va = 'center')\n        else:\n            ax.text(coeff[i,0]* 1.15, coeff[i,1] * 1.15, labels[i], color = 'g', ha = 'center', va = 'center')\n \n    plt.xlabel(\"PC1\")\n    plt.ylabel(\"PC2\")\n    plt.grid()","metadata":{"execution":{"iopub.status.busy":"2023-01-23T00:06:23.123604Z","iopub.execute_input":"2023-01-23T00:06:23.124371Z","iopub.status.idle":"2023-01-23T00:06:23.137351Z","shell.execute_reply.started":"2023-01-23T00:06:23.124323Z","shell.execute_reply":"2023-01-23T00:06:23.135871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"biplot(X_trans[:,0:2],np.transpose(pca.components_[0:2, :]),list(labels),list(states))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-01-23T00:06:23.144669Z","iopub.execute_input":"2023-01-23T00:06:23.145366Z","iopub.status.idle":"2023-01-23T00:06:23.721169Z","shell.execute_reply.started":"2023-01-23T00:06:23.145324Z","shell.execute_reply":"2023-01-23T00:06:23.720107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Each point on a biplot is the projected observation, transformed from the original data. The importance of each feature is indicated by the length of the arrows on the biplot. This corresponds to the magnitude of the values in the eigenvectors. From this biplot, we see that Assault and UrbanPop are the most important features as the arrows to each of these dominate the biplot.\n\n#### This information can also be quantified as follows:","metadata":{}},{"cell_type":"code","source":"# Feature importance\npd.set_option('display.float_format', lambda x: '%.3f' % x) #change precision to see more decimal places\n\npc1 = abs( pca.components_[0] ) #components x features - access at index 0 to get the first component\npc2 = abs( pca.components_[1] )\n\nfeat_df = pd.DataFrame()\nfeat_df[\"Features\"] = list(labels)\nfeat_df[\"PC1 Importance\"] = pc1\nfeat_df[\"PC2 Importance\"] = pc2\nfeat_df","metadata":{"execution":{"iopub.status.busy":"2023-01-23T00:06:23.722774Z","iopub.execute_input":"2023-01-23T00:06:23.723437Z","iopub.status.idle":"2023-01-23T00:06:23.738855Z","shell.execute_reply.started":"2023-01-23T00:06:23.723371Z","shell.execute_reply":"2023-01-23T00:06:23.737637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### From the table above, it seems that the Assault has by far the highest importance in the first principle component, while UrbanPop has the highest important in the second component. These observations agree with those deduced from the biplot. From the summarised statistics from earlier on, the mean values for these features are much larger numbers than those for the other features. It is possible that because of this, these features 'swamp' the others, which results in them being ignored.","metadata":{}},{"cell_type":"markdown","source":"#### Standardised data\n\nStandardise the data so that some features do not swamp the others.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nX_std = StandardScaler().fit_transform(X)\n\nstd_pca = PCA()\nX_std_trans = std_pca.fit_transform(X_std)\n\ndf_std_pca = pd.DataFrame(X_std_trans)\ndf_std_pca.head()","metadata":{"execution":{"iopub.status.busy":"2023-01-23T00:06:23.740285Z","iopub.execute_input":"2023-01-23T00:06:23.740664Z","iopub.status.idle":"2023-01-23T00:06:23.758619Z","shell.execute_reply.started":"2023-01-23T00:06:23.740632Z","shell.execute_reply":"2023-01-23T00:06:23.757292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"biplot(X_std_trans[:,0:2],np.transpose(std_pca.components_[0:2, :]),list(labels))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-01-23T00:06:23.7603Z","iopub.execute_input":"2023-01-23T00:06:23.760715Z","iopub.status.idle":"2023-01-23T00:06:24.323303Z","shell.execute_reply.started":"2023-01-23T00:06:23.760683Z","shell.execute_reply":"2023-01-23T00:06:24.322111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### In the standardised data results abive, far more variables are being utilised to explain the variance, as the large numbers no longer dominate it. ","metadata":{}},{"cell_type":"code","source":"# Feature importance\n\npc1 = abs( std_pca.components_[0] ) #components x features - access at index 0 to get the first component\npc2 = abs( std_pca.components_[1] )\n\nfeat_df = pd.DataFrame()\nfeat_df[\"Features\"] = list(labels)\nfeat_df[\"PC1 Importance\"] = pc1\nfeat_df[\"PC2 Importance\"] = pc2\nfeat_df","metadata":{"execution":{"iopub.status.busy":"2023-01-23T00:06:24.324969Z","iopub.execute_input":"2023-01-23T00:06:24.325393Z","iopub.status.idle":"2023-01-23T00:06:24.344479Z","shell.execute_reply.started":"2023-01-23T00:06:24.325349Z","shell.execute_reply":"2023-01-23T00:06:24.343174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Inspecting the feature importance now, it seems that most of the variables contribute fairly evenly, with only some with low importance.","metadata":{}},{"cell_type":"code","source":"# Cumulative variance plot\nplt.ylabel('Explained variance')\nplt.xlabel('Components')\nplt.plot(range(1,len(std_pca.explained_variance_ratio_ )+1),\n         np.cumsum(std_pca.explained_variance_ratio_),\n         c='red')\nplt.title(\"Cumulative Explained Variance\")","metadata":{"execution":{"iopub.status.busy":"2023-01-23T00:06:24.3462Z","iopub.execute_input":"2023-01-23T00:06:24.346569Z","iopub.status.idle":"2023-01-23T00:06:24.529338Z","shell.execute_reply.started":"2023-01-23T00:06:24.346538Z","shell.execute_reply":"2023-01-23T00:06:24.527821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Scree plot\nplt.plot(std_pca.explained_variance_ratio_)\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance')\nplt.title(\"Scree plot\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-01-23T00:06:24.531048Z","iopub.execute_input":"2023-01-23T00:06:24.531625Z","iopub.status.idle":"2023-01-23T00:06:24.680029Z","shell.execute_reply.started":"2023-01-23T00:06:24.531588Z","shell.execute_reply":"2023-01-23T00:06:24.679231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### From the plots above, it seems the first 3 principal components together explain around 95% of the variance. We can therefore use them to perform cluster analysis. ","metadata":{}},{"cell_type":"code","source":"pca_df = pd.DataFrame(X_std_trans[:,0:3], index = df.index)\npca_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-01-23T00:06:24.681222Z","iopub.execute_input":"2023-01-23T00:06:24.681784Z","iopub.status.idle":"2023-01-23T00:06:24.692138Z","shell.execute_reply.started":"2023-01-23T00:06:24.68175Z","shell.execute_reply":"2023-01-23T00:06:24.691301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## K-Means","metadata":{"tags":[]}},{"cell_type":"code","source":"from sklearn.cluster import KMeans\nkmeans = KMeans(n_clusters = 3)","metadata":{"execution":{"iopub.status.busy":"2023-01-23T00:06:24.693508Z","iopub.execute_input":"2023-01-23T00:06:24.694003Z","iopub.status.idle":"2023-01-23T00:06:24.830439Z","shell.execute_reply.started":"2023-01-23T00:06:24.693948Z","shell.execute_reply":"2023-01-23T00:06:24.829507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Determination of the Optimal Number of Clusters","metadata":{}},{"cell_type":"code","source":"def eval_Kmeans(x, k, r):\n    kmeans = KMeans(n_clusters=k, random_state=r)\n    kmeans.fit(x)\n    return kmeans.inertia_\n\n\ndef elbow_Kmeans(x, max_k=10, r=123):\n    within_cluster_vars = [eval_Kmeans(x, k, r) for k in range(1, max_k+1)]\n    plt.plot(range(1, 11), within_cluster_vars, marker='o')\n    plt.xlabel('K')\n    plt.ylabel('Inertia')\n    plt.show()\n\nx_1 = pca_df[[0,1]]\nX = pca_df[[0,1]].values.reshape(-1, 2)\nelbow_Kmeans(X)","metadata":{"execution":{"iopub.status.busy":"2023-01-23T00:06:24.831734Z","iopub.execute_input":"2023-01-23T00:06:24.832776Z","iopub.status.idle":"2023-01-23T00:06:25.356847Z","shell.execute_reply.started":"2023-01-23T00:06:24.832738Z","shell.execute_reply":"2023-01-23T00:06:25.355464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### The elbow runs from k=2 to k=4. In cases like this, it is not clear which value within the elbow is the most optimal, so we can investigate likely candidates further using the silhouette score.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import silhouette_score\n\ndef scatter_Kmeans(x, k, r=123):\n    ''' This function takes dataframe (x),k and random_state(r) paremeters to build K-Nearest Neighbours model, \n    calculate accuracy of the model and create a scatter plot showing the clusters predicted by the model. \n    '''\n    X = x.values.reshape(-1,2)\n    kmeans = KMeans(n_clusters=k, init='k-means++', random_state=r)\n    y_pred = kmeans.fit_predict(X)\n    colours = 'rbgcmy'\n    \n    for c in range(k):\n      plt.scatter(X[y_pred == c, 0], X[y_pred == c,1], c = colours[c], label = 'Cluster {}'.format(c))\n      plt.scatter(kmeans.cluster_centers_[c, 0], kmeans.cluster_centers_[c, 1], marker='x', c = 'black')\n\n    score = round(silhouette_score(X, kmeans.labels_, metric='euclidean'),2)\n    plt.title('silhouette={}'.format(score), loc='right', fontdict={'fontsize': 16}, pad=-14)\n    plt.xlabel(x.axes[1][0])\n    plt.ylabel(x.axes[1][1])\n    plt.legend()\n    plt.show()\n\nfor k in range(2,5):\n  scatter_Kmeans(x_1, k, r=0)","metadata":{"execution":{"iopub.status.busy":"2023-01-23T00:06:25.358797Z","iopub.execute_input":"2023-01-23T00:06:25.359921Z","iopub.status.idle":"2023-01-23T00:06:26.579831Z","shell.execute_reply.started":"2023-01-23T00:06:25.359876Z","shell.execute_reply":"2023-01-23T00:06:26.578846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Thus we see that the best silhouette score is actually achieved using k=2.","metadata":{}},{"cell_type":"code","source":"from sklearn.cluster import KMeans\n\n# We extract the first two components\nx = X_std_trans[:,0]\ny = X_std_trans[:,1]\n\n# Fit k-means\nk=2\nkmeans = KMeans(n_clusters=k, init='k-means++', random_state=42)\ncluster_labels = kmeans.fit_predict(pca_df)\ncent = kmeans.cluster_centers_\n\n# Plot clusters\nfig, ax = plt.subplots(figsize=(10,10))\ncolours = 'rbgy'\nfor i in range(0,k):\n    ax.scatter(x[cluster_labels == i],y[cluster_labels == i],c = colours[i])\n    ax.scatter(kmeans.cluster_centers_[i, 0], kmeans.cluster_centers_[i, 1], marker='o', c = \"black\", s=300, alpha=0.5)\n\nfor i in range(0,len(x)):\n        txt = states[i]\n        ax.annotate(txt, (x[i], y[i]))\nax.set_title(\"K-Means cluster plot\")\nax.set_xlabel(\"Dim 2\")\nax.set_ylabel(\"Dim 1\")","metadata":{"execution":{"iopub.status.busy":"2023-01-23T00:06:26.582775Z","iopub.execute_input":"2023-01-23T00:06:26.583291Z","iopub.status.idle":"2023-01-23T00:06:27.315548Z","shell.execute_reply.started":"2023-01-23T00:06:26.583231Z","shell.execute_reply":"2023-01-23T00:06:27.314735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Boxplots to display distribution of crime rates for the states in cluster 0 and cluster 1.\ndf['cluster labels']=cluster_labels\nfig, ax = plt.subplots(nrows=1,ncols=3, figsize=(15,5))\nsns.boxplot(x='cluster labels', y='Murder', data=df, ax=ax[0])\nsns.boxplot(x='cluster labels', y='Rape', data=df, ax=ax[1])\nsns.boxplot(x='cluster labels', y='Assault', data=df, ax=ax[2])","metadata":{"execution":{"iopub.status.busy":"2023-01-23T00:06:27.316716Z","iopub.execute_input":"2023-01-23T00:06:27.317794Z","iopub.status.idle":"2023-01-23T00:06:28.055365Z","shell.execute_reply.started":"2023-01-23T00:06:27.31776Z","shell.execute_reply":"2023-01-23T00:06:28.053775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### From the box plots above, it seems\n- The states in Group-0 seems to be Low-risk States where there are relativley less Murders,Assaults and Rapes.\n- The states in the Group-1 seems to have higher crime rates and can be regarded as High-risk States.","metadata":{}},{"cell_type":"code","source":"for i in range(k):\n    group_indices = np.argwhere(cluster_labels==i).transpose()[0]\n    group = np.array(states)[group_indices]\n    print(f'Group {i} States:', *group, sep=\", \")\n    print(\"\\n\")     ","metadata":{"execution":{"iopub.status.busy":"2023-01-23T00:06:28.057202Z","iopub.execute_input":"2023-01-23T00:06:28.058321Z","iopub.status.idle":"2023-01-23T00:06:28.067666Z","shell.execute_reply.started":"2023-01-23T00:06:28.058271Z","shell.execute_reply":"2023-01-23T00:06:28.066329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Agglomerative clustering\nAs with this dataset, the goal of the analysis is to identify the unknown pattern of the data without any prior assumption, agglomerative clustering is a better choice.","metadata":{}},{"cell_type":"code","source":"from sklearn.cluster import AgglomerativeClustering\n\nX = pca_df.values.reshape(-1, 3)\nlinkage_types = ['single', 'complete', 'average', 'ward']\n\nplt.figure(figsize=[15,3])\nfor i, l in enumerate(linkage_types):\n  plt.subplot(1, 4, i+1)\n  cluster = AgglomerativeClustering(n_clusters=2, affinity='euclidean', linkage=l)\n  cluster.fit_predict(X)\n\n  ax = plt.scatter(X[:,0], X[:,1], c=cluster.labels_)\n  plt.title('Clusters based on {} linkage'.format(l))\n \nL = 'complete'","metadata":{"execution":{"iopub.status.busy":"2023-01-23T00:06:28.069426Z","iopub.execute_input":"2023-01-23T00:06:28.070183Z","iopub.status.idle":"2023-01-23T00:06:28.635784Z","shell.execute_reply.started":"2023-01-23T00:06:28.070124Z","shell.execute_reply":"2023-01-23T00:06:28.634491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualisation\nfrom scipy.cluster.hierarchy import dendrogram, linkage\n\nplt.figure(figsize=(20,5))\nplt.title(\"Single linkage based Euclidean metric Dendogram\")\ndend = dendrogram(linkage(X, method='single', metric='euclidean'), labels=pca_df.index)","metadata":{"execution":{"iopub.status.busy":"2023-01-23T00:06:28.637397Z","iopub.execute_input":"2023-01-23T00:06:28.637883Z","iopub.status.idle":"2023-01-23T00:06:30.406029Z","shell.execute_reply.started":"2023-01-23T00:06:28.637839Z","shell.execute_reply":"2023-01-23T00:06:30.404895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print dendogram using complete linkage and euclidean matric\nplt.figure(figsize=(20,5))\nplt.title(\"Complete linkage based Euclidean metric Dendogram\")\ndend = dendrogram(linkage(X, method='complete', metric='euclidean'), labels=pca_df.index)","metadata":{"execution":{"iopub.status.busy":"2023-01-23T00:06:30.407661Z","iopub.execute_input":"2023-01-23T00:06:30.407989Z","iopub.status.idle":"2023-01-23T00:06:32.130275Z","shell.execute_reply.started":"2023-01-23T00:06:30.40796Z","shell.execute_reply":"2023-01-23T00:06:32.129471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print dendogram using average linkage and euclidean matric\nplt.figure(figsize=(20,5))\nplt.title(\"Average linkage based Euclidean metric Dendogram\")\ndend = dendrogram(linkage(X, method='average', metric='euclidean'), labels=pca_df.index)","metadata":{"execution":{"iopub.status.busy":"2023-01-23T00:06:32.13137Z","iopub.execute_input":"2023-01-23T00:06:32.131882Z","iopub.status.idle":"2023-01-23T00:06:33.83523Z","shell.execute_reply.started":"2023-01-23T00:06:32.131847Z","shell.execute_reply":"2023-01-23T00:06:33.83392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# With Ward method\nplt.figure(figsize=(20,5))\nplt.title('Ward linkage based Euclidean metric Dendogram')\ndend = dendrogram(linkage(X, method  = \"ward\"), labels=pca_df.index)","metadata":{"execution":{"iopub.status.busy":"2023-01-23T00:06:33.840539Z","iopub.execute_input":"2023-01-23T00:06:33.840913Z","iopub.status.idle":"2023-01-23T00:06:35.572172Z","shell.execute_reply.started":"2023-01-23T00:06:33.840879Z","shell.execute_reply":"2023-01-23T00:06:35.570976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### From the dendrogram above, it seems there are two clusters.","metadata":{}},{"cell_type":"code","source":"# We extract the first two components\nx = X_std_trans[:,0]\ny = X_std_trans[:,1]\n\n# Run agglomerative hierarchical clustering with 2 number of clusters, \n#'complete' linkage method and 'euclidean' distance metric used for that dendrogram).\ncluster = AgglomerativeClustering(n_clusters=2, affinity='euclidean', linkage='complete')\ncluster_labels = cluster.fit_predict(pca_df)\n\n# Plot clusters\nfig, ax = plt.subplots(figsize=(10,10))\ncolours = 'rbgy'\nfor i in range(0,k):\n    ax.scatter(x[cluster_labels == i],y[cluster_labels == i],c = colours[i]) \n\nfor i in range(0,len(x)):\n        txt = states[i]\n        ax.annotate(txt, (x[i], y[i]))\nax.set_title(\"Agglomerative cluster plot\")\nax.set_xlabel(\"Dim 2\")\nax.set_ylabel(\"Dim 1\")","metadata":{"execution":{"iopub.status.busy":"2023-01-23T00:06:35.573611Z","iopub.execute_input":"2023-01-23T00:06:35.573924Z","iopub.status.idle":"2023-01-23T00:06:36.252141Z","shell.execute_reply.started":"2023-01-23T00:06:35.573895Z","shell.execute_reply":"2023-01-23T00:06:36.250715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Verify the clusters obtained by using the Silhouette score \nkmeans = KMeans(n_clusters=2, init='k-means++', random_state=1)\ny_pred = kmeans.fit_predict(X)\nscore = round(silhouette_score(X, kmeans.labels_, metric='euclidean'), 2)\nscore","metadata":{"execution":{"iopub.status.busy":"2023-01-23T00:06:36.253632Z","iopub.execute_input":"2023-01-23T00:06:36.253959Z","iopub.status.idle":"2023-01-23T00:06:36.278778Z","shell.execute_reply.started":"2023-01-23T00:06:36.25393Z","shell.execute_reply":"2023-01-23T00:06:36.277354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Boxplots to display distribution of crime rates for the states in cluster 0 and cluster 1.\ndf['cluster labels']=cluster_labels\nfig, ax = plt.subplots(nrows=1,ncols=3, figsize=(15,5))\nsns.boxplot(x='cluster labels', y='Murder', data=df, ax=ax[0])\nsns.boxplot(x='cluster labels', y='Rape', data=df, ax=ax[1])\nsns.boxplot(x='cluster labels', y='Assault', data=df, ax=ax[2])","metadata":{"execution":{"iopub.status.busy":"2023-01-23T00:06:36.280646Z","iopub.execute_input":"2023-01-23T00:06:36.281452Z","iopub.status.idle":"2023-01-23T00:06:36.75112Z","shell.execute_reply.started":"2023-01-23T00:06:36.281387Z","shell.execute_reply":"2023-01-23T00:06:36.749969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### From the box plots above, it seems\n- The states in the Group-0 seems to have higher crime rates and can be regarded as High-risk States.\n- The states in Group-1 seems to be Low-risk States where there are relativley less Murders,Assaults and Rapes.","metadata":{}},{"cell_type":"code","source":"for i in range(k):\n    group_indices = np.argwhere(cluster_labels==i).transpose()[0]\n    group = np.array(states)[group_indices]\n    print(f'Group {i} States:', *group, sep=\", \")\n    print(\"\\n\")","metadata":{"execution":{"iopub.status.busy":"2023-01-23T00:06:36.752578Z","iopub.execute_input":"2023-01-23T00:06:36.753123Z","iopub.status.idle":"2023-01-23T00:06:36.759979Z","shell.execute_reply.started":"2023-01-23T00:06:36.75309Z","shell.execute_reply":"2023-01-23T00:06:36.758923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}